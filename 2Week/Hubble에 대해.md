# Hubble이란?
Hubble은 완전히 분산된 네트워크 및 보안 ‘관측(Observability)’ 플랫폼
Cilium과 eBPF 기술을 기반으로 서비스 간 통신, 동작, 네트워크 인프라 등에 대해 투명하고 상세한 가시성을 제공

# 핵심 기술 및 아키텍처
* eBPF 기반 동적 가시성: Cilium 위에서 동작하며, eBPF를 활용해 필요한 네트워크 가시성을 프로그래밍적으로 구현하고, 성능 오버헤드를 최소화
* 클러스터/노드/다중클러스터 가시성: 한 노드, 클러스터 전체, 여러 클러스터(메시 환경) 모두에서 네트워크 흐름과 보안 상태를 관측

# 주요 구성
* Hubble API: 개별 Cilium 에이전트 포드에 내장되어, 그 노드에서 관찰된 트래픽 및 상태 정보를 제공(로컬 범위).
* Hubble CLI: 로컬 Hubble API를 유닉스 도메인 소켓을 통해 쿼리하여 네트워크 정보를 출력하는 기본 명령줄 도구.
* Hubble Relay: 클러스터~클러스터, 전체 클러스터 수준의 가시성을 제공하며, Hubble CLI 또는 Hubble UI가 Hubble Relay를 통해 전체 클러스터 데이터를 접근할 수 있게 해줌.
* Hubble UI: 웹 기반 시각화 도구로, L3/L4 (IP/Port)뿐 아니라 L7(HTTP 등)에서 서비스 간 종속성/트래픽 관계를 그래픽 맵으로 보여줌.

# Cilium & Hubble 의 대표 기능 

eBPF는 시스템과 애플리케이션에 대한 정밀하고 효율적인 가시성 및 제어 기능을 완전히 투명한 방식으로 제공하며, 애플리케이션의 수정 없이도 작동합니다. 이는 컨테이너 기반 워크로드뿐만 아니라 전통적인 가상 머신 및 리눅스 프로세스에도 적합합니다.

현재 데이터센터 애플리케이션은 종종 마이크로서비스 아키텍처로 구성되며, 이는 대규모 애플리케이션을 작고 독립적인 서비스로 분할하여 HTTP와 같은 경량 프로토콜로 통신합니다. 이러한 환경에서는 컨테이너가 수시로 생성 및 종료되며, 이는 보안 및 네트워크 연결 가시성 측면에서 새로운 도전 과제를 야기합니다.

기존의 iptables 기반 보안 접근 방식은 IP 주소와 포트를 기반으로 동작하므로, 마이크로서비스 환경의 빈번하게 변경되는 IP 주소와 함께 사용하기에는 확장성에 한계가 있습니다.
Cilium은 eBPF를 활용하여 서비스/Pod/컨테이너의 ID 기반 보안 정책 적용, HTTP 레벨의 애플리케이션 필터링, L3~L7 계층에 걸친 분리와 제어를 제공함으로써 동적 환경에서도 강력한 보안을 제공합니다.

* 서비스 종속성 & 통신 맵
	* 어떤 서비스들이 서로 통신하는가? 얼마나 자주? 전체 구조/그래프는?
	* 어떤 HTTP 호출이 오가는가? Kafka의 경우 어떤 topic을 사용하고 있는가?
		* - `GET /public/.*` 경로에 대한 요청만 허용하고 나머지 거부
		- Kafka topic1에 대해 서비스1은 생산만, 서비스2는 소비만 허용
		- 모든 REST 호출에 `X-Token: [0-9]+` 헤더 필수
* 네트워크 모니터링 & 경보
	* 어떤 네트워크 통신에 장애가 발생하고 있는가? 원인은?
	* DNS 문제, 애플리케이션 문제, 혹은 네트워크 자체 문제인가?
	* 최근 어떤 서비스가 DNS 문제를 겪었는가? TCP 연결 중단/타임아웃은? TCP SYN 응답률은?
* 애플리케이션 모니터링
	* 5xx, 4xx HTTP 응답이 서비스별/클러스터 전체 비율은?
	* HTTP 지연 시간의 95/99 퍼센타일은 얼마인가? 가장 느린 서비스는 무엇인가? 서비스 간 지연은 얼마나 되는가?
* 보안 관측 기능
	* 어떤 서비스가 네트워크 정책에 의해 접속 차단됐는가?
		* ID 기반 서비스 간 보안 통신
			* 동일한 보안 정책을 공유하는 컨테이너 그룹에 보안 ID를 부여하고, 네트워크 패킷에 ID를 포함하여 수신 측에서 검증
	* 외부(클러스터 외부)에서 접근된 서비스는?
		* CIDR 기반 정책을 통해 외부 IP 범위에 대한 인바운드 및 아웃바운드 접근 제어 가능
	* 어떤 서비스에서 특정 DNS 이름이 resolve 됐는가?
* 로드밸런싱
	* eBPF 기반 - 분산 로드밸런싱으로 kube-proxy 대체 가능
	- XDP, DSR, Maglev 해싱 지원
	- 커널 소켓 계층에서 서비스-백엔드 매핑 처리로 NAT 오버헤드 제거


# 통신 방식 
## 핵심 컴포넌트
* Cilium + Hubble Server: 각 쿠버네티스 노드에 설치되어, 해당 노드의 네트워크 트래픽·이벤트를 수집
* Hubble Relay: 클러스터 내 모든 Hubble Server 인스턴스와 gRPC로 연결해 각종 네트워크 이벤트(관찰 데이터)를 중앙에서 집계
* Hubble CLI & UI: Hubble Relay를 통해 집계된 데이터를 API로 받아 커맨드라인 또는 웹 UI에 시각화/검색 기능 제공.
* ClusterMesh: 여러 클러스터의 Hubble Relay 인스턴스들이 상호 연결되어, 클러스터 간 네트워크 흐름까지 통합적으로 관측할 수 있도록 지원

## 단일 클러스터
1.	각각의 노드(Cilium Agent)는 자신의 Hubble Server를 실행합니다.
2.	Hubble Server들은 노드에서 포집한 네트워크·보안 이벤트를 Hubble Relay로 gRPC 스트림 방식으로 전달합니다.
3.	Hubble Relay는 각 노드로부터 받은 데이터를 집계하여 API로 공개합니다.
4.	사용자는 Hubble CLI나 Hubble UI를 통해 Relay에 접근해 네트워크 상태, 서비스 간 통신, 보안 이벤트 등 클러스터 전반의 실시간 정보·시각화 데이터를 확인할 수 있습니다.
## 멀티 클러스터(ClusterMesh 시나리오)
1.	각 클러스터에는 별도의 Hubble Relay가 구성됩니다.
2.	ClusterMesh 기능을 활성화하면, 여러 클러스터의 Relay들이 서로 연결되어 클러스터 간 네트워크 이벤트도 통합적으로 집계·표현합니다.
3.	이를 통해 여러 클러스터 간의 트래픽 흐름, 정책 위반, 서비스 맵 등을 한 곳(Hubble UI/CLI)에서 통합 모니터링할 수 있습니다.