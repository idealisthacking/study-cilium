NodeLocalDNS ì†Œê°œ [https://popappend.tistory.com/142](https://popappend.tistory.com/142)

Using NodeLocal DNSCache in Kubernetes Clusters ì†Œê°œ - [Docs](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/)
![[Pasted image 20250728004910.png]]
- `NodeLocal DNSCache`ëŠ” í´ëŸ¬ìŠ¤í„° ë…¸ë“œì—ì„œ DNS ìºì‹± ì—ì´ì „íŠ¸ë¥¼ DaemonSetìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ í´ëŸ¬ìŠ¤í„° DNS ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
- ì˜¤ëŠ˜ë‚ ì˜ ì•„í‚¤í…ì²˜ì—ì„œ '`ClusterFirst`' DNS ëª¨ë“œì˜ PodsëŠ” DNS ì¿¼ë¦¬ë¥¼ ìœ„í•´ kube-dns ì„œë¹„ìŠ¤ IPì— ë„ë‹¬í•©ë‹ˆë‹¤.
- ì´ëŠ” kube-proxyì— ì˜í•´ ì¶”ê°€ëœ **iptables** ê·œì¹™ì„ í†µí•´ kube-dns/CoreDNS ì—”ë“œí¬ì¸íŠ¸ë¡œ ë³€í™˜ë©ë‹ˆë‹¤.
- ì´ ìƒˆë¡œìš´ ì•„í‚¤í…ì²˜ë¥¼ í†µí•´ PodsëŠ” ë™ì¼í•œ ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ëŠ” DNS ìºì‹± ì—ì´ì „íŠ¸ì— ë„ë‹¬í•˜ì—¬ iptables DNAT ê·œì¹™ê³¼ ì—°ê²° ì¶”ì ì„ í”¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ë¡œì»¬ ìºì‹± ì—ì´ì „íŠ¸ëŠ” í´ëŸ¬ìŠ¤í„° í˜¸ìŠ¤íŠ¸ ì´ë¦„(ê¸°ë³¸ì ìœ¼ë¡œ â€œcluster.localâ€ ì ‘ë¯¸ì‚¬)ì˜ ìºì‹œ ëˆ„ë½ì— ëŒ€í•´ kube-dns ì„œë¹„ìŠ¤ì— ì¿¼ë¦¬í•©ë‹ˆë‹¤.
- í˜„ì¬ DNS ì•„í‚¤í…ì²˜ì—ì„œëŠ” ë¡œì»¬ kube-dns/CoreDNS ì¸ìŠ¤í„´ìŠ¤ê°€ ì—†ëŠ” ê²½ìš° DNS QPSê°€ ê°€ì¥ ë†’ì€ í¬ë“œê°€ ë‹¤ë¥¸ ë…¸ë“œì— ë„ë‹¬í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ë¡œì»¬ ìºì‹œë¥¼ ì‚¬ìš©í•˜ë©´ ì´ëŸ¬í•œ ì‹œë‚˜ë¦¬ì˜¤ì—ì„œ ì§€ì—° ì‹œê°„ì„ ê°œì„ í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
- iptables DNAT ë° ì—°ê²° ì¶”ì ì„ ê±´ë„ˆë›°ë©´ [ì—°ê²° ì¶”ì  ë ˆì´ìŠ¤](https://github.com/kubernetes/kubernetes/issues/56903)ë¥¼ ì¤„ì´ê³  UDP DNS í•­ëª©ì´ ì—°ê²° ì¶”ì  í…Œì´ë¸”ì„ ì±„ìš°ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.
- ë¡œì»¬ ìºì‹± ì—ì´ì „íŠ¸ì—ì„œ kube-dns ì„œë¹„ìŠ¤ë¡œì˜ ì—°ê²°ì€ TCPë¡œ ì—…ê·¸ë ˆì´ë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. TCP ì—°ê²° íŠ¸ë™ í•­ëª©ì€ ì‹œê°„ ì´ˆê³¼ë¥¼ í•´ì•¼ í•˜ëŠ” UDP í•­ëª©ê³¼ ë‹¬ë¦¬ ì—°ê²° ì¢…ë£Œ ì‹œ ì œê±°ë©ë‹ˆë‹¤(ê¸°ë³¸ê°’ `nf_conntrack_udp_timeout`ì€ 30ì´ˆ)
- DNS ì¿¼ë¦¬ë¥¼ UDPì—ì„œ TCPë¡œ ì—…ê·¸ë ˆì´ë“œí•˜ë©´ ì‚­ì œëœ UDP íŒ¨í‚·ê³¼ DNS íƒ€ì„ì•„ì›ƒìœ¼ë¡œ ì¸í•œ í…Œì¼ ì§€ì—° ì‹œê°„ì´ ë³´í†µ ìµœëŒ€ 30ì´ˆ(3íšŒ ì¬ì‹œë„ + 10ì´ˆ íƒ€ì„ì•„ì›ƒ)ê¹Œì§€ ì¤„ì–´ë“­ë‹ˆë‹¤. ë…¸ë“œë¡œì»¬ ìºì‹œê°€ UDP DNS ì¿¼ë¦¬ë¥¼ ë“£ê¸° ë•Œë¬¸ì— ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë³€ê²½í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.
- ë…¸ë“œ ìˆ˜ì¤€ì—ì„œ DNS ìš”ì²­ì— ëŒ€í•œ ë©”íŠ¸ë¦­ ë° ê°€ì‹œì„±.
- ë„¤ê±°í‹°ë¸Œ ìºì‹±ì„ ë‹¤ì‹œ í™œì„±í™”í•˜ì—¬ kube-dns ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì¿¼ë¦¬ ìˆ˜ë¥¼ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.


# NodeLocal DNSCache ì„¤ì¹˜ - [Docs](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#configuration)

- NodeLocal DSCacheì˜ ë¡œì»¬ ì²­ì·¨ IP ì£¼ì†ŒëŠ” í´ëŸ¬ìŠ¤í„°ì˜ ê¸°ì¡´ IPì™€ ì¶©ëŒí•˜ì§€ ì•Šë„ë¡ ë³´ì¥í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ì£¼ì†Œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ì˜ˆë¥¼ ë“¤ì–´, IPv4ì˜ 'ë§í¬-ë¡œì»¬' ë²”ìœ„ '`169.254.0.0/16`' ë˜ëŠ” IPv6 '`fd00::/8`'ì˜ 'ìœ ë‹ˆí¬ ë¡œì»¬ ì£¼ì†Œ' ë²”ìœ„ì™€ ê°™ì€ ë¡œì»¬ ë²”ìœ„ì˜ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.
- Prepare a manifest similar to the sampleÂ [`nodelocaldns.yaml`](https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml)Â and save it asÂ `nodelocaldns.yaml`.
- **Substitute the variables in the manifest with the right values:**

```bash
kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}` # coredns ì˜ ClusterIP
domain=<cluster-domain> # ë³´í†µ ê¸°ë³¸ê°’ cluster.local ì‚¬ìš©
localdns=<node-local-address> # local listen IP address chosen for NodeLocal DNSCache
```

If kube-proxy is running in IPTABLES mode:
```bash
#
sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml

- `__PILLAR__CLUSTER__DNS__`Â andÂ `__PILLAR__UPSTREAM__SERVERS__`ëŠ” `node-local-dns` í¬ë“œì— ì˜í•´ ì±„ì›Œì§‘ë‹ˆë‹¤.
- ì´ ëª¨ë“œì—ì„œëŠ” `node-local-dns` í¬ë“œê°€ **kube-dns ì„œë¹„ìŠ¤ IP**ì™€ **<node-local-address>**ë¥¼ ëª¨ë‘ ìˆ˜ì‹ í•˜ë¯€ë¡œ, í¬ë“œëŠ” IP ì£¼ì†Œ ì¤‘ í•˜ë‚˜ë¥¼ ì‚¬ìš©í•˜ì—¬ DNS ë ˆì½”ë“œë¥¼ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
```



If kube-proxy is running in IPVS mode:
```bash
#
sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/,__PILLAR__DNS__SERVER__//g; s/__PILLAR__CLUSTER__DNS__/$kubedns/g" nodelocaldns.yaml

- ì´ ëª¨ë“œì—ì„œëŠ”`node-local-dns` í¬ë“œê°€ **<node-local-address>**ì—ì„œë§Œ ì²­ì·¨í•©ë‹ˆë‹¤.
- IPVS ë¡œë“œ ë°¸ëŸ°ì‹±ì— ì‚¬ìš©ë˜ëŠ” ì¸í„°í˜ì´ìŠ¤ê°€ ì´ë¯¸ ì´ ì£¼ì†Œë¥¼ ì‚¬ìš©í•˜ê³  ìˆê¸° ë•Œë¬¸ì— `node-local-dns` ì¸í„°í˜ì´ìŠ¤ëŠ” kube-dns í´ëŸ¬ìŠ¤í„° IPë¥¼ ë°”ì¸ë”©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
- `__PILLAR__UPSTREAM__SERVERS__`ëŠ” `node-local-dns` í¬ë“œì— ì˜í•´ ì±„ì›Œì§‘ë‹ˆë‹¤.
```

- **ë°°í¬** Â `kubectl create -f nodelocaldns.yaml`
- `node-local-dns` í¬ë“œê°€ í™œì„±í™”ë˜ë©´ ê° í´ëŸ¬ìŠ¤í„° ë…¸ë“œì˜ `kube-system` ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì‹¤í–‰ë©ë‹ˆë‹¤.
- ì´ í¬ë“œëŠ” ìºì‹œ ëª¨ë“œì—ì„œ CoreDNSë¥¼ ì‹¤í–‰í•˜ë¯€ë¡œ ì„œë¡œ ë‹¤ë¥¸ í”ŒëŸ¬ê·¸ì¸ì´ ë…¸ì¶œí•˜ëŠ” ëª¨ë“  CoreDNS ë©”íŠ¸ë¦­ì„ ë…¸ë“œ ë‹¨ìœ„ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- `kubectl delete -f <manifest>`ë¥¼ ì‚¬ìš©í•˜ì—¬ DaemonSetì„ ì œê±°í•˜ì—¬ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë³€ê²½í•œ ë‚´ìš©ì„ kubetle ì„¤ì •ìœ¼ë¡œ ë˜ëŒë ¤ì•¼ í•©ë‹ˆë‹¤.
- StubDomains and upstream servers specified in the kube-dns ConfigMap in the kube-system namespace are automatically picked up by node-local-dns pods.


# (ì°¸ê³ ) Setting memory limits : ìºì‹œ ì—”íŠ¸ë¦¬ì™€ ì¿¼ë¦¬ ì²˜ë¦¬ì— ë©”ëª¨ë¦¬ ì‚¬ìš©. 10000ê°œ ì—”íŠ¸ë¦¬(30MB ë©”ëª¨ë¦¬), max_concurrent â†’ VPA - [Docs](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#setting-memory-limits)

- TheÂ `node-local-dns`Â Pods use **memory** for storing **cache entries** and **processing queries**. Since they do not watch Kubernetes objects, the cluster size or the number of Services / EndpointSlices do not directly affect memory usage. **Memory** **usage** is influenced by the DNS query pattern. FromÂ [CoreDNS docs](https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md),
    
    <aside> ğŸ‘‰ğŸ»
    
    The default cache size is **10000** **entries**, which uses about **30 MB** when completely filled.
    
    </aside>
    
- This would be the memory usage for each server block (if the cache gets completely filled). Memory usage can be reduced by specifying smaller cache sizes.
    
- The number of concurrent queries is linked to the memory demand, because each extra goroutine used for handling a query requires an amount of memory. You can set an upper limit using theÂ `max_concurrent`Â option in the forward plugin.
    
- If aÂ `node-local-dns`Â Pod attempts to use more memory than is available (because of total system resources, or because of a configuredÂ [resource limit](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/)), the operating system may shut down that pod's container. If this happens, the container that is terminated (â€œOOMKilledâ€) does not clean up the custom packet filtering rules that it previously added during startup. TheÂ `node-local-dns`Â container should get restarted (since managed as part of a DaemonSet), but this will lead to a brief DNS downtime each time that the container fails: the packet filtering rules direct DNS queries to a local Pod that is unhealthy.
    
- You can determine a suitable memory limit by running node-local-dns pods without a limit and measuring the peak usage. You can also set up and use aÂ [VerticalPodAutoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)Â inÂ _recommender mode_, and then check its recommendations.


# NodeLocalDNS ì‹¤ìŠµ + Cilium Local Redirect Policy

^bd1807

NodeLocal DNSCache ì„¤ì¹˜ ë° í™•ì¸ - [Docs](https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/#configuration) , [Github](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/dns/nodelocaldns)
```bash
# iptables í™•ì¸
iptables-save | tee before.txt

#
wget https://github.com/kubernetes/kubernetes/raw/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml

# kubedns ëŠ” coredns ì„œë¹„ìŠ¤ì˜ ClusterIPë¥¼ ë³€ìˆ˜ ì§€ì •
kubedns=`kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP}`
domain='cluster.local'    ## default ê°’
localdns='169.254.20.10'  ## default ê°’
echo $kubedns $domain $localdns

# iptables ëª¨ë“œ ì‚¬ìš© ì¤‘ìœ¼ë¡œ ì•„ë˜ ëª…ë ¹ì–´ ìˆ˜í–‰
sed -i "s/__PILLAR__LOCAL__DNS__/$localdns/g; s/__PILLAR__DNS__DOMAIN__/$domain/g; s/__PILLAR__DNS__SERVER__/$kubedns/g" nodelocaldns.yaml

# nodelocaldns ì„¤ì¹˜
kubectl apply -f nodelocaldns.yaml

#
kubectl get pod -n kube-system -l k8s-app=node-local-dns -owide
NAME                   READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
node-local-dns-9xrv9   1/1     Running   0          41s   192.168.10.100   k8s-ctr   <none>           <none>

#
kubectl edit cm -n kube-system node-local-dns # 'cluster.local' ê³¼ '.:53' ì— log, debug ì¶”ê°€
kubectl -n kube-system rollout restart ds node-local-dns

kubectl describe cm -n kube-system node-local-dns
...
cluster.local:53 {
    errors
    cache {
            success 9984 30
            denial 9984 5
    }
    reload
    loop
    bind 169.254.20.10 10.96.0.10
    forward . __PILLAR__CLUSTER__DNS__ {
            force_tcp
    }
    prometheus :9253
    health 169.254.20.10:8080
    }
    ...
.:53 {
    errors
    cache 30
    reload
    loop
    bind 169.254.20.10 10.96.0.10
    forward . __PILLAR__UPSTREAM__SERVERS__
    prometheus :9253
    }
...

# iptables í™•ì¸ : ê·œì¹™ ì—…ë°ì´íŠ¸ê¹Œì§€ ë‹¤ì†Œ ì‹œê°„ ì†Œìš”!
iptables-save | tee after.txt
diff before.txt after.txt

##
iptables -t filter -S | grep -i dns
-A INPUT -d 10.96.0.10/32 -p udp -m udp --dport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A INPUT -d 10.96.0.10/32 -p tcp -m tcp --dport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A INPUT -d 169.254.20.10/32 -p udp -m udp --dport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A INPUT -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A OUTPUT -s 10.96.0.10/32 -p udp -m udp --sport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A OUTPUT -s 10.96.0.10/32 -p tcp -m tcp --sport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A OUTPUT -s 169.254.20.10/32 -p udp -m udp --sport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT
-A OUTPUT -s 169.254.20.10/32 -p tcp -m tcp --sport 53 -m comment --comment "NodeLocal DNS Cache: allow DNS traffic" -j ACCEPT

##
iptables -t raw -S | grep -i dns
-A PREROUTING -d 10.96.0.10/32 -p udp -m udp --dport 53 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
-A PREROUTING -d 10.96.0.10/32 -p tcp -m tcp --dport 53 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
-A PREROUTING -d 169.254.20.10/32 -p udp -m udp --dport 53 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
-A PREROUTING -d 169.254.20.10/32 -p tcp -m tcp --dport 53 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
-A OUTPUT -s 10.96.0.10/32 -p tcp -m tcp --sport 8080 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
-A OUTPUT -d 10.96.0.10/32 -p tcp -m tcp --dport 8080 -m comment --comment "NodeLocal DNS Cache: skip conntrack" -j NOTRACK
...

# logs : 
kubectl -n kube-system logs -l k8s-app=kube-dns -f
kubectl -n kube-system logs -l k8s-app=node-local-dns -f

#
kubectl exec -it curl-pod -- cat /etc/resolv.conf
search default.svc.cluster.local svc.cluster.local cluster.local
nameserver 10.96.0.10
options ndots:5

# 
kubectl exec -it curl-pod -- nslookup webpod
kubectl exec -it curl-pod -- nslookup google.com

#
kubectl delete pod curl-pod

cat << EOF | kubectl apply -f -
apiVersion: v1
kind: Pod
metadata:
  name: curl-pod
  labels:
    app: curl
spec:
  containers:
  - name: curl
    image: nicolaka/netshoot
    command: ["tail"]
    args: ["-f", "/dev/null"]
  terminationGracePeriodSeconds: 0
EOF

kubectl exec -it curl-pod -- cat /etc/resolv.conf

# ë¡œê·¸ í™•ì¸ ì‹œ í˜„ì¬ nodelocaldns ë¯¸í™œìš©! 
kubectl -n kube-system logs -l k8s-app=kube-dns -f
kubectl -n kube-system logs -l k8s-app=node-local-dns -f

#
kubectl exec -it curl-pod -- nslookup webpod
kubectl exec -it curl-pod -- nslookup google.com

```


# Cilium Local Redirect Policy : --set localRedirectPolicy=true - [Docs](https://docs.cilium.io/en/stable/network/kubernetes/local-redirect-policy/)

- IP ì£¼ì†Œì™€ Port/Protocol tuple ë˜ëŠ” **Kubernetes Service** ë¡œ í–¥í•˜ëŠ” í¬ë“œ **íŠ¸ë˜í”½**ì„ eBPFë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¸ë“œ ë‚´ **ë°±ì—”ë“œ í¬ë“œë¡œ ë¡œì»¬ë¡œ ë¦¬ë””ë ‰ì…˜**í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” Ciliumì˜ ë¡œì»¬ ë¦¬ë””ë ‰ì…˜ ì •ì±…ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.
- ë°±ì—”ë“œ í¬ë“œì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì •ì±…ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
- CiliumLocalRedirectPolicyëŠ” CustomResourceDefinitionìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
node-local-dns configmap ë‚´ìš©
```bash 
Corefile:
----
cluster.local:53 {
    log
    debug
    errors
    cache {
            success 9984 30
            denial 9984 5
    }
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__CLUSTER__DNS__ {
            force_tcp
    }
    prometheus :9253
    health
    }
in-addr.arpa:53 {
    errors
    cache 30
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__CLUSTER__DNS__ {
            force_tcp
    }
    prometheus :9253
    }
ip6.arpa:53 {
    errors
    cache 30
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__CLUSTER__DNS__ {
            force_tcp
    }
    prometheus :9253
    }
.:53 {
    log
    debug
    errors
    cache 30
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__UPSTREAM__SERVERS__
    prometheus :9253
    }
```

```bash
#
helm upgrade cilium cilium/cilium --namespace kube-system --reuse-values \
  --set localRedirectPolicy=true

kubectl rollout restart deploy cilium-operator -n kube-system
kubectl rollout restart ds cilium -n kube-system

#
wget https://raw.githubusercontent.com/cilium/cilium/1.17.6/examples/kubernetes-local-redirect/node-local-dns.yaml

kubedns=$(kubectl get svc kube-dns -n kube-system -o jsonpath={.spec.clusterIP})
sed -i "s/__PILLAR__DNS__SERVER__/$kubedns/g;" node-local-dns.yaml
vi -d nodelocaldns.yaml node-local-dns.yaml

## before
args: [ "-localip", "169.254.20.10,10.96.0.10", "-conf", "/etc/Corefile", "-upstreamsvc", "kube-dns-upstream" ]

## after
args: [ "-localip", "169.254.20.10,10.96.0.10", "-conf", "/etc/Corefile", "-upstreamsvc", "kube-dns-upstream", "-skipteardown=true", "-setupinterface=false", "-setupiptables=false" ]


# ë°°í¬
# Modify Node-local DNS cacheâ€™s deployment yaml to pass these additional arguments to node-cache: 
## -skipteardown=true, -setupinterface=false, and -setupiptables=false.

# Modify Node-local DNS cacheâ€™s deployment yaml to put it in non-host namespace by setting hostNetwork: false for the daemonset.
# In the Corefile, bind to 0.0.0.0 instead of the static IP.
kubectl apply -f node-local-dns.yaml

#
kubectl edit cm -n kube-system node-local-dns # log, debug ì¶”ê°€
kubectl -n kube-system rollout restart ds node-local-dns

kubectl describe cm -n kube-system node-local-dns
cluster.local:53 {
    errors
    cache {
            success 9984 30
            denial 9984 5
    }
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__CLUSTER__DNS__ {
            force_tcp
    }
    prometheus :9253
    health
    }
    ...
.:53 {
    errors
    cache 30
    reload
    loop
    bind 0.0.0.0
    forward . __PILLAR__UPSTREAM__SERVERS__
    prometheus :9253
    }


#
wget https://raw.githubusercontent.com/cilium/cilium/1.17.6/examples/kubernetes-local-redirect/node-local-dns-lrp.yaml
cat node-local-dns-lrp.yaml | yq
apiVersion: "cilium.io/v2"
kind: CiliumLocalRedirectPolicy
metadata:
  name: "nodelocaldns"
  namespace: kube-system
spec:
  redirectFrontend:
    serviceMatcher:
      serviceName: kube-dns
      namespace: kube-system
  redirectBackend:
    localEndpointSelector:
      matchLabels:
        k8s-app: node-local-dns
    toPorts:
      - port: "53"
        name: dns
        protocol: UDP
      - port: "53"
        name: dns-tcp
        protocol: TCP
        
kubectl apply -f https://raw.githubusercontent.com/cilium/cilium/1.17.6/examples/kubernetes-local-redirect/node-local-dns-lrp.yaml

#
kubectl get CiliumLocalRedirectPolicy -A
NAMESPACE     NAME           AGE
kube-system   nodelocaldns   5m26s

#
kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg lrp list
LRP namespace   LRP name       FrontendType                Matching Service
kube-system     nodelocaldns   clusterIP + all svc ports   kube-system/kube-dns
                |              10.96.0.10:53/UDP -> 172.20.0.52:53(kube-system/node-local-dns-mhv4p), 
                |              10.96.0.10:53/TCP -> 172.20.0.52:53(kube-system/node-local-dns-mhv4p), 
                |              10.96.0.10:9153/TCP -> 

kubectl exec -it -n kube-system ds/cilium -c cilium-agent -- cilium-dbg service list | grep LocalRedirect
2    10.96.0.10:53/UDP     LocalRedirect   1 => 172.20.0.52:53/UDP (active)        
3    10.96.0.10:53/TCP     LocalRedirect   1 => 172.20.0.52:53/TCP (active)


# logs
kubectl -n kube-system logs -l k8s-app=kube-dns -f
kubectl -n kube-system logs -l k8s-app=node-local-dns -f

#
kubectl exec -it curl-pod -- nslookup www.google.com


# nodelocaldns ì— ìºì‹œëœ ì •ë³´ë¡œ ë°”ë¡œ ì§ˆì˜ ì‘ë‹µ í™•ì¸!
kubectl -n kube-system logs -l k8s-app=node-local-dns -f
[INFO] 172.20.0.119:59966 - 48193 "A IN www.google.com.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.057756155s
[INFO] 172.20.0.119:57117 - 36265 "A IN www.google.com.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000990079s
[INFO] 172.20.0.119:57568 - 25713 "A IN www.google.com.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000896198s
[INFO] 172.20.0.119:58204 - 42574 "A IN www.google.com. udp 32 false 512" NOERROR qr,rd,ra 62 0.035130807s
[INFO] 172.20.0.119:46673 - 7830 "AAAA IN www.google.com. udp 32 false 512" NOERROR qr,rd,ra 74 0.007996413s


```


## (ì°¸ê³ ) cilium ì¬ì„¤ì¹˜ : --set cleanBpfState=true --set cleanState=true
```bash
#
helm uninstall -n kube-system cilium

#
iptables-save | grep -v KUBE | grep -v CILIUM | iptables-restore
iptables-save

sshpass -p 'vagrant' ssh vagrant@k8s-w1 "sudo iptables-save | grep -v KUBE | grep -v CILIUM | sudo iptables-restore"
sshpass -p 'vagrant' ssh vagrant@k8s-w1 sudo iptables-save

#
helm install cilium cilium/cilium --version 1.17.6 --namespace kube-system \
--set k8sServiceHost=192.168.10.100 --set k8sServicePort=6443 \
--set ipam.mode="cluster-pool" --set ipam.operator.clusterPoolIPv4PodCIDRList={"172.20.0.0/16"} --set ipv4NativeRoutingCIDR=172.20.0.0/16 \
--set routingMode=native --set autoDirectNodeRoutes=true --set endpointRoutes.enabled=true \
--set kubeProxyReplacement=true --set bpf.masquerade=true --set installNoConntrackIptablesRules=true \
--set endpointHealthChecking.enabled=false --set healthChecking=false \
--set hubble.enabled=true --set hubble.relay.enabled=true --set hubble.ui.enabled=true \
--set hubble.ui.service.type=NodePort --set hubble.ui.service.nodePort=30003 \
--set prometheus.enabled=true --set operator.prometheus.enabled=true --set hubble.metrics.enableOpenMetrics=true \
--set hubble.metrics.enabled="{dns,drop,tcp,flow,port-distribution,icmp,httpV2:exemplars=true;labelsContext=source_ip\,source_namespace\,source_workload\,destination_ip\,destination_namespace\,destination_workload\,traffic_direction}" \
--set operator.replicas=1 --set debug.enabled=true --set cleanBpfState=true --set cleanState=true

#
k9s â†’ pod â†’ 0 (all) 
```