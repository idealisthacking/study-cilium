# 실습 환경 설명
![[Pasted image 20250719211139.png]]

- 기본 배포 가상 머신 : k8s-ctr, k8s-w1, k8s-w2
    - eth0 : 10.0.2.15 모든 노드가 동일
    - eth1 : 192.168.10.100, 101, 102
- 초기 프로비저닝으로 kubeadm init 과 join 실행됨
- CNI 미설치 상태로 배포 완료됨


# 실습 프로그램 및 배포 파일 

## VirtualBox 설치 - [Release](https://www.virtualbox.org/wiki/Changelog)
```bash
    # VirtualBox 설치
    brew install --cask virtualbox
    
    VBoxManage --version
    7.1.10r169112
```

## Vagrant 설치
```bash
    # Vagrant 설치
    brew install --cask vagrant
    
    vagrant version    
    Installed Version: 2.4.7
```

## Vagrantfile : 가상머신 정의, 부팅 시 초기 프로비저닝 설정
```bash
# Variables
K8SV = '1.33.2-1.1' # Kubernetes Version : apt list -a kubelet , ex) 1.32.5-1.1
CONTAINERDV = '1.7.27-1' # Containerd Version : apt list -a containerd.io , ex) 1.6.33-1
N = 2 # max number of worker nodes

# Base Image  <https://portal.cloud.hashicorp.com/vagrant/discover/bento/ubuntu-24.04>
## Rocky linux Image <https://portal.cloud.hashicorp.com/vagrant/discover/rockylinux>
BOX_IMAGE = "bento/ubuntu-24.04"
#BOX_IMAGE = "rockylinux/8"
BOX_VERSION = "202502.21.0"
#BOX_VERSION = "10.0.0"

Vagrant.configure("2") do |config|
#-ControlPlane Node
    config.vm.define "k8s-ctr" do |subconfig|
      subconfig.vm.box = BOX_IMAGE
      subconfig.vm.box_version = BOX_VERSION
      subconfig.vm.provider "virtualbox" do |vb|
        vb.customize ["modifyvm", :id, "--groups", "/Cilium-Lab"]
        vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all"]
        vb.name = "k8s-ctr"
        vb.cpus = 2
        vb.memory = 2048
        vb.linked_clone = true
      end
      subconfig.vm.host_name = "k8s-ctr"
      subconfig.vm.network "private_network", ip: "192.168.10.100"
      subconfig.vm.network "forwarded_port", guest: 22, host: 60000, auto_correct: true, id: "ssh"
      subconfig.vm.synced_folder "./", "/vagrant", disabled: true
      subconfig.vm.provision "shell", path: "init_cfg.sh", args: [ K8SV, CONTAINERDV]
      subconfig.vm.provision "shell", path: "k8s-ctr.sh", args: [ N ]
    end

#-Worker Nodes Subnet1
  (1..N).each do |i|
    config.vm.define "k8s-w#{i}" do |subconfig|
      subconfig.vm.box = BOX_IMAGE
      subconfig.vm.box_version = BOX_VERSION
      subconfig.vm.provider "virtualbox" do |vb|
        vb.customize ["modifyvm", :id, "--groups", "/Cilium-Lab"]
        vb.customize ["modifyvm", :id, "--nicpromisc2", "allow-all"]
        vb.name = "k8s-w#{i}"
        vb.cpus = 2
        vb.memory = 1536
        vb.linked_clone = true
      end
      subconfig.vm.host_name = "k8s-w#{i}"
      subconfig.vm.network "private_network", ip: "192.168.10.10#{i}"
      subconfig.vm.network "forwarded_port", guest: 22, host: "6000#{i}", auto_correct: true, id: "ssh"
      subconfig.vm.synced_folder "./", "/vagrant", disabled: true
      subconfig.vm.provision "shell", path: "init_cfg.sh", args: [ K8SV, CONTAINERDV]
      subconfig.vm.provision "shell", path: "k8s-w.sh"
    end
  end

end

```

## init_cfg.sh : args 참고하여 설치
```bash
#!/usr/bin/env bash

echo ">>>> Initial Config Start <<<<"

echo "[TASK 1] Setting Profile & Change Timezone"
echo 'alias vi=vim' >> /etc/profile
echo "sudo su -" >> /home/vagrant/.bashrc
ln -sf /usr/share/zoneinfo/Asia/Seoul /etc/localtime


echo "[TASK 2] Disable AppArmor"
systemctl stop ufw && systemctl disable ufw >/dev/null 2>&1
systemctl stop apparmor && systemctl disable apparmor >/dev/null 2>&1


echo "[TASK 3] Disable and turn off SWAP"
swapoff -a && sed -i '/swap/s/^/#/' /etc/fstab


echo "[TASK 4] Install Packages"
apt update -qq >/dev/null 2>&1
apt-get install apt-transport-https ca-certificates curl gpg -y -qq >/dev/null 2>&1

# Download the public signing key for the Kubernetes package repositories.
mkdir -p -m 755 /etc/apt/keyrings
K8SMMV=$(echo $1 | sed -En 's/^([0-9]+\.[0-9]+)\..*/\1/p')
curl -fsSL https://pkgs.k8s.io/core:/stable:/v$K8SMMV/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v$K8SMMV/deb/ /" >> /etc/apt/sources.list.d/kubernetes.list
curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

# packets traversing the bridge are processed by iptables for filtering
echo 1 > /proc/sys/net/ipv4/ip_forward
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.d/k8s.conf

# enable br_netfilter for iptables 
modprobe br_netfilter
modprobe overlay
echo "br_netfilter" >> /etc/modules-load.d/k8s.conf
echo "overlay" >> /etc/modules-load.d/k8s.conf


echo "[TASK 5] Install Kubernetes components (kubeadm, kubelet and kubectl)"
# Update the apt package index, install kubelet, kubeadm and kubectl, and pin their version
apt update >/dev/null 2>&1

# apt list -a kubelet ; apt list -a containerd.io
apt-get install -y kubelet=$1 kubectl=$1 kubeadm=$1 containerd.io=$2 >/dev/null 2>&1
apt-mark hold kubelet kubeadm kubectl >/dev/null 2>&1

# containerd configure to default and cgroup managed by systemd
containerd config default > /etc/containerd/config.toml
sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml

# avoid WARN&ERRO(default endpoints) when crictl run  
cat <<EOF > /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
EOF

# ready to install for k8s 
systemctl restart containerd && systemctl enable containerd
systemctl enable --now kubelet


echo "[TASK 6] Install Packages & Helm"
apt-get install -y bridge-utils sshpass net-tools conntrack ngrep tcpdump ipset arping wireguard jq tree bash-completion unzip kubecolor >/dev/null 2>&1
curl -s https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash >/dev/null 2>&1


echo ">>>> Initial Config End <<<<"

```

## k8s-ctr.sh : kubeadm init (Pod/ServiceCIDR) , 편리성 설정(k, kc)
```bash
#!/usr/bin/env bash

echo ">>>> K8S Controlplane config Start <<<<"

echo "[TASK 1] Initial Kubernetes"
kubeadm init --config="/tmp/kubeadm-init-ctr-config.yaml" >/dev/null 2>&1

echo "[TASK 2] Setting kube config file"
mkdir -p /root/.kube
cp -i /etc/kubernetes/admin.conf /root/.kube/config
chown $(id -u):$(id -g) /root/.kube/config


echo "[TASK 3] Source the completion"
echo 'source <(kubectl completion bash)' >> /etc/profile
echo 'source <(kubeadm completion bash)' >> /etc/profile


echo "[TASK 4] Alias kubectl to k"
echo 'alias k=kubectl' >> /etc/profile
echo 'alias kc=kubecolor' >> /etc/profile
echo 'complete -F __start_kubectl k' >> /etc/profile


echo "[TASK 5] Install Kubectx & Kubens"
git clone https://github.com/ahmetb/kubectx /opt/kubectx >/dev/null 2>&1
ln -s /opt/kubectx/kubens /usr/local/bin/kubens
ln -s /opt/kubectx/kubectx /usr/local/bin/kubectx


echo "[TASK 6] Install Kubeps & Setting PS1"
git clone https://github.com/jonmosco/kube-ps1.git /root/kube-ps1 >/dev/null 2>&1
cat <<"EOT" >> /root/.bash_profile
source /root/kube-ps1/kube-ps1.sh
KUBE_PS1_SYMBOL_ENABLE=true
function get_cluster_short() {
  echo "$1" | cut -d . -f1
}
KUBE_PS1_CLUSTER_FUNCTION=get_cluster_short
KUBE_PS1_SUFFIX=') '
PS1='$(kube_ps1)'$PS1
EOT
kubectl config rename-context "kubernetes-admin@kubernetes" "HomeLab" >/dev/null 2>&1


echo "[TASK 6] Install Kubeps & Setting PS1"
echo "192.168.10.100 k8s-ctr" >> /etc/hosts
for (( i=1; i<=$1; i++  )); do echo "192.168.10.10$i k8s-w$i" >> /etc/hosts; done


echo ">>>> K8S Controlplane Config End <<<<"
```

## k8s-w.sh : kubeadm join
```bash
#!/usr/bin/env bash

echo ">>>> K8S Node config Start <<<<"

WORKER_NUM=$1
WORKER_IP="192.168.10.10${WORKER_NUM}"

echo "[TASK 1] K8S Controlplane Join"
sed -i "s/NODE_IP/${WORKER_IP}/g" /tmp/kubeadm-join-worker-config.yaml
kubeadm join --config="/tmp/kubeadm-join-worker-config.yaml" > /dev/null 2>&1

echo ">>>> K8S Node config End <<<<"
```


```bash
mkdir cilium-lab && cd cilium-lab

curl -O <https://raw.githubusercontent.com/gasida/vagrant-lab/refs/heads/main/cilium-study/1w/Vagrantfile>

vagrant up
```

-------------
# 도전 과제

^451032

vagrant box 를 Rocky linux 로 실습 환경 배포 해보기

```
BOX_IMAGE = "rockylinux/9"
BOX_VERSION = "6.0.0"
```

```
SSH 접속 후 Rocky Linux 및 커널 정보 확인
`vagrant ssh k8s-ctr`
`cat /etc/os-release`, `uname -r` 등
각 노드별 kubeadm 등 정상 설치 여부 확인
```
# MacOS Trouble Shooting 
![[Pasted image 20250719191206.png]]
```
There was an error while executing `VBoxManage`, a CLI used by Vagrant
for controlling VirtualBox. The command and stderr is shown below.

Command: ["startvm", "23d525c1-6ce3-469d-ac2c-21431f39c2cb", "--type", "headless"]

Stderr: VBoxManage: error: The VM session was aborted
VBoxManage: error: Details: code NS_ERROR_FAILURE (0x80004005), component SessionMachine, interface ISession


1. VirtualBox 커널 드라이버 미로딩 또는 권한 문제

MacOS에서는 VirtualBox가 필요로 하는 커널 확장(드라이버)이 부팅 시 제대로 로딩되지 않으면 이 오류가 발생하기 쉽습니다.

시스템 환경설정 → 보안 및 개인정보 보호(Security & Privacy) → 일반(General) 탭에서 "Oracle"의 시스템 소프트웨어 허용 메시지가 보이면 반드시 '허용'해 주세요, 그리고 시스템을 재부팅 해야 합니다.
```
-------
## 실습환경 구성 후, ssh 접속
배포 후 ssh 접속 : vagrant ssh k8s-ctr , vagrant ssh k8s-w1 , vagrant ssh k8s-w2
```bash
# ssh 접속 전, 노드들의 eth0 IP 확인
for i in ctr w1 w2 ; do echo ">> node : k8s-$i <<"; vagrant ssh k8s-$i -c 'ip -c -4 addr show dev eth0'; echo; done 
```
![[Pasted image 20250719212545.png]]

### [k8s-ctr] 접속 후 기본 정보 확인
```bash
#
whoami
pwd
hostnamectl
htop

#
cat /etc/hosts
ping -c 1 k8s-w1
ping -c 1 k8s-w2
sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-w1 hostname
sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-w2 hostname

# vagrant ssh 로 접속 시 tcp 연결 정보 : NAT Mode 10.0.2.2(GateWay)
ss -tnp |grep sshd
ESTAB 0      0           [::ffff:10.0.2.15]:22          [::ffff:10.0.2.2]:52791 users:(("sshd",pid=5176,fd=4),("sshd",pid=5129,fd=4))

# nic 정보
ip -c addr

# default 라우팅 정보 
ip -c route

# dns 서버 정보 : NAT Mode 10.0.2.3
resolvectl

```

### [k8s-ctr] k8s 정보 확인
```bash
# 
kubectl cluster-info

# 노드 정보 : 상태, INTERNAL-IP 확인
kubectl get node -owide

# 파드 정보 : 상태, 파드 IP 확인 - kube-proxy 확인
kubectl get pod -A -owide

# 단축어 확인(kc = kubecolor) & coredns 파드 상태 확인
k  describe pod -n kube-system -l k8s-app=kube-dns
kc describe pod -n kube-system -l k8s-app=kube-dns
```
![[Pasted image 20250719212842.png]]


### [k8s-ctr] INTERNAL-IP 변경 설정
```bash
#
cat /var/lib/kubelet/kubeadm-flags.env

# INTERNAL-IP 변경 설정
NODEIP=$(ip -4 addr show eth1 | grep -oP '(?<=inet\s)\d+(\.\d+){3}')

sed -i "s/^\(KUBELET_KUBEADM_ARGS=\"\)/\1--node-ip=${NODEIP} /" /var/lib/kubelet/kubeadm-flags.env

systemctl daemon-reexec && systemctl restart kubelet

cat /var/lib/kubelet/kubeadm-flags.env

#
kubectl get node -owide
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   NotReady   control-plane   24m   v1.33.2   192.168.10.100   <none>        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
...
```
### [k8s-w1/w2] INTERNAL-IP 변경 설정 할 것 → 설정 완료 후 아래 처럼 확인
```bash
kubectl get node -owide
NAME      STATUS     ROLES           AGE   VERSION   INTERNAL-IP      EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME
k8s-ctr   NotReady   control-plane   26m   v1.33.2   192.168.10.100   <none>        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w1    NotReady   <none>          24m   v1.33.2   192.168.10.101   <none>        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27
k8s-w2    NotReady   <none>          23m   v1.33.2   192.168.10.102   <none>        Ubuntu 24.04.2 LTS   6.8.0-53-generic   containerd://1.7.27

kubectl get pod -A -owide
NAMESPACE     NAME                              READY   STATUS    RESTARTS   AGE   IP               NODE      NOMINATED NODE   READINESS GATES
kube-system   coredns-674b8bbfcf-fl4tr          0/1     Pending   0          26m   <none>           <none>    <none>           <none>
kube-system   coredns-674b8bbfcf-npvs9          0/1     Pending   0          26m   <none>           <none>    <none>           <none>
kube-system   etcd-k8s-ctr                      1/1     Running   0          26m   10.0.2.15        k8s-ctr   <none>           <none>
kube-system   kube-apiserver-k8s-ctr            1/1     Running   0          26m   10.0.2.15        k8s-ctr   <none>           <none>
kube-system   kube-controller-manager-k8s-ctr   1/1     Running   0          26m   10.0.2.15        k8s-ctr   <none>           <none>
kube-system   kube-proxy-bwwjq                  1/1     Running   0          23m   192.168.10.102   k8s-w2    <none>           <none>
kube-system   kube-proxy-hqqq7                  1/1     Running   0          26m   192.168.10.100   k8s-ctr   <none>           <none>
kube-system   kube-proxy-l9t87                  1/1     Running   0          24m   192.168.10.101   k8s-w1    <none>           <none>
kube-system   kube-scheduler-k8s-ctr            1/1     Running   0          26m   10.0.2.15        k8s-ctr   <none>           <none>
```
![[Pasted image 20250719213314.png]]
### [k8s-ctr] static pod 의 IP 변경 설정
```bash
#
tree /etc/kubernetes/manifests
/etc/kubernetes/manifests
├── etcd.yaml
├── kube-apiserver.yaml
├── kube-controller-manager.yaml
└── kube-scheduler.yaml

# etcd 정보 확인
cat /etc/kubernetes/manifests/etcd.yaml
...
  volumes:
  - hostPath:
	  path: /etc/kubernetes/pki/etcd
	  type: DirectoryOrCreate
	name: etcd-certs
  - hostPath:
	  path: /var/lib/etcd
	  type: DirectoryOrCreate
	name: etcd-data

tree /var/lib/etcd/
/var/lib/etcd/
└── member
	├── snap
	│   └── db
	└── wal
		├── 0000000000000000-0000000000000000.wal
		└── 0.tmp

```
    
### `vagrant ssh k8s-ctr` 재접속 후 확인
    
```bash
kubectl get pod -n kube-system -owide
```

--------
# 도전과제

^2853f8

kubeadm Configuration 를 사용해서 node-ip 를 지정해서 init/join 해보자



```bash
# kubeadm-init.yaml
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
nodeRegistration:
  kubeletExtraArgs:
    node-ip: "192.168.10.100"
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
kubernetesVersion: v1.33.2
apiServer:
  certSANs:
    - "192.168.10.100"
networking:
  podSubnet: "10.244.0.0/16"
  serviceSubnet: "10.96.0.0/16"
controlPlaneEndpoint: "192.168.10.100:6443"


# kubeadm-join.yaml
# 각 워커노드마다 자신의 IP에 맞춰 node-ip를 수정
apiVersion: kubeadm.k8s.io/v1beta4
kind: JoinConfiguration
discovery:
  bootstrapToken:
    token: "123456.1234567890123456"
    apiServerEndpoint: "192.168.10.100:6443"
    unsafeSkipCAVerification: true
nodeRegistration:
  kubeletExtraArgs:
    - name: node-ip
      value: "NODE_IP"
    #node-ip: "192.168.10.101"   # example for k8s-w1
    #node-ip: "192.168.10.102"   # example for k8s-w2

```

```
vagrant ssh k8s-ctr
vagrant ssh k8s-w1
vagrant ssh k8s-w2
kubectl get node -owide
INTERNAL-IP가 private 네트워크 대역으로 노출되는지 확인
```

## 확인
* [k8s-ctr] 접속 후 기본 정보 확인
```
#
cat /etc/hosts
sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-w1 hostname
sshpass -p 'vagrant' ssh -o StrictHostKeyChecking=no vagrant@k8s-w2 hostname

#
ifconfig | grep -iEA1 'eth[0-9]:'

# 클러스터 정보 확인
kubectl cluster-info
kubectl cluster-info dump | grep -m 2 -E "cluster-cidr|service-cluster-ip-range"
kubectl describe cm -n kube-system kubeadm-config
kubectl describe cm -n kube-system kubelet-config

# 노드 정보 : 상태, INTERNAL-IP 확인
kubectl get node -owide

# 노드별 kubeadm-flags.env 정보 확인
cat /var/lib/kubelet/kubeadm-flags.env
for i in w1 w2 ; do echo ">> node : k8s-$i <<"; sshpass -p 'vagrant' ssh vagrant@k8s-$i cat /var/lib/kubelet/kubeadm-flags.env ; echo; done

# 파드 정보 : 상태, 파드 IP 확인
kubectl get nodes -o jsonpath='{range .items[*]}{.metadata.name}{"\t"}{.spec.podCIDR}{"\n"}{end}'
kubectl get ciliumnode -o json | grep podCIDRs -A2
kubectl get pod -A -owide

# iptables 확인
iptables-save
iptables -t nat -S
iptables -t filter -S
iptables -t mangle -S
```

* [k8s-ctr] cilium 설치 정보 확인
```
# cilium 상태 확인
which cilium
cilium status
cilium config view
kubectl get cm -n kube-system cilium-config -o json | jq

#
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg config
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg status --verbose
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg metrics list

#
kubectl get ciliumendpoints -A

# monitor
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor -v
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor -v -v

## Filter for only the events related to endpoint
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor --related-to=<id>

## Show notifications only for dropped packet events
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor --type drop

## Don’t dissect packet payload, display payload in hex information
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor -v -v --hex

## Layer7
kubectl exec -n kube-system -c cilium-agent -it ds/cilium -- cilium-dbg monitor -v --type l7
```

-------

# 실습환경 삭제
```
vagrant destroy -f && rm -rf .vagrant
```